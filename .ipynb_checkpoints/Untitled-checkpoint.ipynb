{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5386de3-22e8-4786-866e-f10921f06e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import __version__ as matplotlib_version\n",
    "\n",
    "\n",
    "# Produce a testbed of 2000 bandit problems\n",
    "np.random.seed(10)\n",
    "n_bandit = 2000\n",
    "n_lever = 10\n",
    "steps = 1000\n",
    "\n",
    "testbed = np.random.normal(0,1,(n_bandit,n_lever))\n",
    "initial_reward_estimates =  np.random.normal(testbed,1)\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "def rewards_per_step_mp(steps, n_bandit, n_lever, testbed, initial_reward_estimates, epsilon,global_reward_list):\n",
    "    action_count = np.ones((n_bandit,n_lever))  # each lever is pulled atleast once\n",
    "    reward_estimates = np.zeros((n_bandit,n_lever))  # _per_pull_per_bandit\n",
    "\n",
    "    avg_rewards_per_step = []\n",
    "    avg_rewards_per_step.append(0)\n",
    "    avg_rewards_per_step.append(np.mean(initial_reward_estimates)) # step 1\n",
    "\n",
    "    for step in range(2,steps):\n",
    "        reward_sum_over_all_bandits_per_step = 0\n",
    "        for problem_index in range(n_bandit):  \n",
    "            if np.random.random() > epsilon:\n",
    "                maxval = np.amax(reward_estimates[problem_index])\n",
    "                maxval_indices = np.ravel(np.array(np.where(reward_estimates[problem_index] == maxval)))\n",
    "                random_choice = np.random.choice( maxval_indices ) # Breaking ties randomly\n",
    "            else :\n",
    "                 random_choice = np.random.randint(n_lever)\n",
    "\n",
    "            # Incremental Algorithm\n",
    "            Rn = np.random.normal(testbed[problem_index][random_choice],1)\n",
    "            n = action_count[problem_index] [random_choice]\n",
    "            Qn = reward_estimates[problem_index][random_choice]\n",
    "            \n",
    "            Qnew = ( Rn + (n - 1) * Qn ) / n\n",
    "            \n",
    "            reward_estimates[problem_index][random_choice] = Qnew\n",
    "            action_count[problem_index] [random_choice] = n+1\n",
    "            \n",
    "            reward_sum_over_all_bandits_per_step += Qnew\n",
    "            \n",
    "\n",
    "        avg_rewards_per_step.append((reward_sum_over_all_bandits_per_step)/n_bandit)\n",
    "        \n",
    "    global_reward_list.append(avg_rewards_per_step)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91ebdfc-c56a-4821-ac89-d50c24f812e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with multiprocessing.Manager() as manager:\n",
    "    global_reward_list = []\n",
    "    mp_objects = []\n",
    "    epsilon_list= [0.1]\n",
    "    color_list = ['r']\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xlabel('Steps', fontsize=18)\n",
    "    plt.ylabel('Average Reward', fontsize=16)\n",
    "    plt.plot(np.arange(steps),global_reward_list[index][0],label=epsilon_list[index], color=color_list[index])\n",
    "    \n",
    "    plt.legend(loc=2, ncol=2)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee544f4-42a2-4b7b-826b-94c310f9406a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97e1144-2bc0-45f9-9a00-fcc433b4615d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
