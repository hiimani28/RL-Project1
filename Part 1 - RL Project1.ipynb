{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ede5f94",
   "metadata": {},
   "source": [
    "## Part 1-1: Greedy with non-optimistic values (Incremental implementation of simple average method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4580f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random \n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7821548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_per_step_greedy(steps, n_bandit, n_lever, testbed, global_reward_list, global_optimal_action_list):\n",
    "    action_count = np.zeros((n_bandit, n_lever))  \n",
    "    reward_estimates = np.zeros((n_bandit, n_lever))  # initial reward estimates set to zero\n",
    "    avg_rewards_per_step = []\n",
    "    optimal_action_count = np.zeros(steps)\n",
    "\n",
    "    for step in range(steps):\n",
    "        reward_sum_over_all_bandits_per_step = 0\n",
    "        optimal_action_chosen_count = 0\n",
    "\n",
    "        for problem_index in range(n_bandit):  \n",
    "            maxval = np.amax(reward_estimates[problem_index]) # find the maximum value of the reward for that problem.\n",
    "            maxval_indices = np.ravel(np.where(reward_estimates[problem_index] == maxval)) #gets the index of that reward in the problem\n",
    "            # print(\"maxval and maxval_indices\", maxval, maxval_indices)\n",
    "            random_choice = np.random.choice(maxval_indices)  #in situatuin where there are multiple rewards with the same value\n",
    "            #: Randomly selects one index from the list of indices with the highest estimated reward. This breaks the tie randomly among the levers that have the same highest estimated reward.\n",
    "            # print(\"random choice is\" , random_choice)\n",
    "\n",
    "            # Generate reward from the testbed\n",
    "            Rn = np.random.normal(testbed[problem_index][random_choice], 1)\n",
    "            action_count[problem_index][random_choice] += 1\n",
    "            n = action_count[problem_index][random_choice]\n",
    "            \n",
    "            # To check if the action is never taken \n",
    "            if n==0:\n",
    "                reward_estimates[problem_index][random_choice] = 0\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                # Update reward estimate using the incremental implementation of the simple average method\n",
    "                reward_estimates[problem_index][random_choice] += (Rn - reward_estimates[problem_index][random_choice]) / n\n",
    "            \n",
    "            reward_sum_over_all_bandits_per_step += Rn\n",
    "            \n",
    "            # Check if the optimal action was chosen\n",
    "            optimal_action = np.argmax(testbed[problem_index])\n",
    "            if random_choice == optimal_action:\n",
    "                optimal_action_chosen_count += 1\n",
    "\n",
    "        avg_rewards_per_step.append(reward_sum_over_all_bandits_per_step / n_bandit)\n",
    "        optimal_action_count[step] = optimal_action_chosen_count / n_bandit\n",
    "\n",
    "    global_reward_list.append(avg_rewards_per_step)\n",
    "    global_optimal_action_list.append(optimal_action_count)\n",
    "    return\n",
    "\n",
    "# Parameters\n",
    "steps = 100\n",
    "n_bandit = 100\n",
    "n_lever = 10\n",
    "\n",
    "# Testbed: 1000 sets of ten mean parameters\n",
    "testbed = np.random.normal(0, 1, (n_bandit, n_lever))\n",
    "\n",
    "global_reward_list = []\n",
    "global_optimal_action_list = []\n",
    "\n",
    "rewards_per_step_greedy(steps, n_bandit, n_lever, testbed, global_reward_list, global_optimal_action_list)\n",
    "\n",
    "# Average reward at each time step\n",
    "average_reward_across_runs = np.mean(global_reward_list, axis=0)\n",
    "\n",
    "# Percentage of time the optimal action is taken\n",
    "optimal_action_percentage = np.mean(global_optimal_action_list, axis=0) * 100\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot average reward\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(average_reward_across_runs, label='Average Reward')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward vs. Steps')\n",
    "plt.legend()\n",
    "\n",
    "# Plot percentage of optimal action\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(optimal_action_percentage, label='% Optimal Action')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('% Optimal Action')\n",
    "plt.title('% Optimal Action vs. Steps')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23a4d1d",
   "metadata": {},
   "source": [
    "## Part 1-2: Epsilon Greedy with different choices of epsilon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cbeeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_per_step_greedy(steps, n_bandit, n_lever, testbed, epsilon):\n",
    "    action_count = np.zeros((n_bandit, n_lever))  \n",
    "    reward_estimates = np.zeros((n_bandit, n_lever))  # initial reward estimates set to zero\n",
    "    avg_rewards_per_step = []\n",
    "    optimal_action_count = np.zeros(steps)\n",
    "\n",
    "    for step in range(steps):\n",
    "        reward_sum_over_all_bandits_per_step = 0\n",
    "        optimal_action_chosen_count = 0\n",
    "\n",
    "        for problem_index in range(n_bandit):  \n",
    "            var_random = random.random()\n",
    "            if (var_random > epsilon):\n",
    "                maxval = np.amax(reward_estimates[problem_index]) # find the maximum value of the reward for that problem.\n",
    "                maxval_indices = np.ravel(np.where(reward_estimates[problem_index] == maxval)) #gets the index of that reward in the problem\n",
    "                random_choice = np.random.choice(maxval_indices)  #in situatuin where there are multiple rewards with the same value\n",
    "                # Randomly selects one index from the list of indices with the highest estimated reward.\n",
    "            else:\n",
    "                random_choice = np.random.randint(n_lever)\n",
    "\n",
    "            # Generate reward from the testbed\n",
    "            Rn = np.random.normal(testbed[problem_index][random_choice], 1)\n",
    "            action_count[problem_index][random_choice] += 1\n",
    "            n = action_count[problem_index][random_choice]\n",
    "            \n",
    "            # To check if the action is never taken \n",
    "            if n==0:\n",
    "                reward_estimates[problem_index][random_choice] = 0\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                # Update reward estimate using the incremental implementation of the simple average method\n",
    "                reward_estimates[problem_index][random_choice] += (Rn - reward_estimates[problem_index][random_choice]) / n\n",
    "\n",
    "            reward_sum_over_all_bandits_per_step += Rn\n",
    "\n",
    "            # Check if the optimal action was chosen\n",
    "            optimal_action = np.argmax(testbed[problem_index])\n",
    "            if random_choice == optimal_action:\n",
    "                optimal_action_chosen_count += 1\n",
    "\n",
    "        avg_rewards_per_step.append(reward_sum_over_all_bandits_per_step / n_bandit)\n",
    "        optimal_action_count[step] = optimal_action_chosen_count / n_bandit\n",
    "\n",
    "    return avg_rewards_per_step, optimal_action_count\n",
    "\n",
    "# Parameters\n",
    "steps = 1000\n",
    "n_bandit = 1000\n",
    "n_lever = 10\n",
    "epsilon_values = [0.0, 0.01, 0.1, 0.2, 0.5]  # 5 different epsilon values\n",
    "colors = ['r', 'g', 'b', 'k', 'y']  \n",
    "\n",
    "# Testbed: 1000 sets of ten mean parameters\n",
    "testbed = np.random.normal(0, 1, (n_bandit, n_lever))\n",
    "\n",
    "# To store results\n",
    "all_avg_rewards = []\n",
    "all_optimal_action_percentages = []\n",
    "\n",
    "for epsilon in epsilon_values:\n",
    "    avg_rewards_per_step, optimal_action_count = rewards_per_step_greedy(steps, n_bandit, n_lever, testbed, epsilon)\n",
    "    all_avg_rewards.append(avg_rewards_per_step)\n",
    "    all_optimal_action_percentages.append(optimal_action_count * 100)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot average reward\n",
    "plt.subplot(2, 1, 1)\n",
    "for i, epsilon in enumerate(epsilon_values):\n",
    "    plt.plot(all_avg_rewards[i], label=f'ε = {epsilon}', color=colors[i])\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward vs. Steps')\n",
    "plt.legend()\n",
    "\n",
    "# Plot percentage of optimal action\n",
    "plt.subplot(2, 1, 2)\n",
    "for i, epsilon in enumerate(epsilon_values):\n",
    "    plt.plot(all_optimal_action_percentages[i], label=f'ε = {epsilon}', color=colors[i])\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('% Optimal Action')\n",
    "plt.title('% Optimal Action vs. Steps')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29f0145",
   "metadata": {},
   "source": [
    "## Part 1-3: Optimistic Starting values with a greedy approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90d50e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_per_step_optimistic_greedy(steps, n_bandit, n_lever, testbed, epsilon):\n",
    "    action_count = np.zeros((n_bandit, n_lever))  \n",
    "    reward_estimates = np.ones((n_bandit, n_lever)) * 5  # initial reward estimates set to five \n",
    "    avg_rewards_per_step = []\n",
    "    optimal_action_count = np.zeros(steps)\n",
    "\n",
    "    for step in range(steps):\n",
    "        reward_sum_over_all_bandits_per_step = 0\n",
    "        optimal_action_chosen_count = 0\n",
    "\n",
    "        for problem_index in range(n_bandit):  \n",
    "            var_random = random.random()\n",
    "            if (var_random > epsilon):\n",
    "                maxval = np.amax(reward_estimates[problem_index]) # find the maximum value of the reward for that problem.\n",
    "                maxval_indices = np.ravel(np.where(reward_estimates[problem_index] == maxval)) #gets the index of that reward in the problem\n",
    "                random_choice = np.random.choice(maxval_indices)  #in situatuin where there are multiple rewards with the same value\n",
    "                # Randomly selects one index from the list of indices with the highest estimated reward.\n",
    "            else:\n",
    "                random_choice = np.random.randint(n_lever)\n",
    "\n",
    "            # Generate reward from the testbed\n",
    "            Rn = np.random.normal(testbed[problem_index][random_choice], 1)\n",
    "            action_count[problem_index][random_choice] += 1\n",
    "            n = action_count[problem_index][random_choice]\n",
    "\n",
    "            # To check if the action is never taken \n",
    "            if n==0:\n",
    "                reward_estimates[problem_index][random_choice] = 0\n",
    "            \n",
    "            else:\n",
    "                # Update reward estimate using the incremental implementation of the simple average method\n",
    "                reward_estimates[problem_index][random_choice] += (Rn - reward_estimates[problem_index][random_choice]) / n\n",
    "\n",
    "            reward_sum_over_all_bandits_per_step += Rn\n",
    "\n",
    "            # Check if the optimal action was chosen\n",
    "            optimal_action = np.argmax(testbed[problem_index])\n",
    "            if random_choice == optimal_action:\n",
    "                optimal_action_chosen_count += 1\n",
    "\n",
    "        avg_rewards_per_step.append(reward_sum_over_all_bandits_per_step / n_bandit)\n",
    "        optimal_action_count[step] = optimal_action_chosen_count / n_bandit\n",
    "\n",
    "    return avg_rewards_per_step, optimal_action_count\n",
    "\n",
    "# Parameters\n",
    "steps = 1000\n",
    "n_bandit = 1000\n",
    "n_lever = 10\n",
    "epsilon_values = [0.0, 0.01, 0.1, 0.2, 0.5]  # 5 different epsilon values\n",
    "colors = ['r', 'g', 'b', 'k', 'y']  \n",
    "\n",
    "# Testbed: 1000 sets of ten mean parameters\n",
    "testbed = np.random.normal(0, 1, (n_bandit, n_lever))\n",
    "\n",
    "# To store results\n",
    "all_avg_rewards = []\n",
    "all_optimal_action_percentages = []\n",
    "\n",
    "for epsilon in epsilon_values:\n",
    "    avg_rewards_per_step, optimal_action_count = rewards_per_step_optimistic_greedy(steps, n_bandit, n_lever, testbed, epsilon)\n",
    "    all_avg_rewards.append(avg_rewards_per_step)\n",
    "    all_optimal_action_percentages.append(optimal_action_count * 100)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot average reward\n",
    "plt.subplot(2, 1, 1)\n",
    "for i, epsilon in enumerate(epsilon_values):\n",
    "    plt.plot(all_avg_rewards[i], label=f'ε = {epsilon}', color=colors[i])\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Average Reward')\n",
    "plt.title('Average Reward vs. Steps')\n",
    "plt.legend()\n",
    "\n",
    "# Plot percentage of optimal action\n",
    "plt.subplot(2, 1, 2)\n",
    "for i, epsilon in enumerate(epsilon_values):\n",
    "    plt.plot(all_optimal_action_percentages[i], label=f'ε = {epsilon}', color=colors[i])\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('% Optimal Action')\n",
    "plt.title('% Optimal Action vs. Steps')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71f0b61",
   "metadata": {},
   "source": [
    "## Part 1-4: Gradient Bandit Algorithm (Different Learning rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7e2eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for softmax distribution\n",
    "def softmax_distribution(preference_estimates):\n",
    "    maxval = np.amax(preference_estimates)\n",
    "    exps = np.exp(preference_estimates - maxval)  # Using property of softmax function\n",
    "    probabilities = exps / np.sum(exps, axis=0)\n",
    "    return probabilities\n",
    "\n",
    "# Gradient Bandit Algorithm\n",
    "def gradient_bandit(steps, n_bandit, n_lever, step_size, is_baseline_applied,\n",
    "                    testbed, global_average_reward_list, optimal_choice_list):\n",
    "    \n",
    "    action_count = np.zeros((n_bandit, n_lever))  \n",
    "    Ravg = np.zeros((n_bandit, n_lever))  # avg_reward_estimates\n",
    "    Hpref = np.zeros((n_bandit, n_lever))  # preference_estimates\n",
    "    pr_action_t = np.zeros((n_bandit, n_lever))  # probability of action a at time t\n",
    "\n",
    "    optimal_choice_per_step = []\n",
    "    mean_reward = 0\n",
    "    rewards = np.zeros((steps, n_bandit))\n",
    "    for step in range(steps):\n",
    "        sum_of_optimal_choice = 0 \n",
    "        avg_reward = []\n",
    "        # Loop through all problems \n",
    "        for b in range(n_bandit):  \n",
    "            # calculating the probability of actions based on their preference \n",
    "            pr_action_t[b] = softmax_distribution(Hpref[b])\n",
    "            # chooisesq a random action from n_lever(possible action) according to their probabilities \n",
    "            A = np.random.choice(np.arange(n_lever), p=pr_action_t[b])\n",
    "\n",
    "            # np.arg max will return the index of the maximum value of the problem bandit from testbed and compare it with A (action NOT reward)\n",
    "            if A == np.argmax(testbed[b]):\n",
    "                # Add 1 if the condition is true \n",
    "                sum_of_optimal_choice += 1 \n",
    "            # Calculate the reward based on testbed[problem][action]\n",
    "            Rn = np.random.normal(testbed[b][A], 1)\n",
    "            rewards[step,b] = Rn\n",
    "            \n",
    "            if is_baseline_applied:\n",
    "                n = step + 1\n",
    "                mean_reward = (Rn + (n - 1) * mean_reward) / n\n",
    "            \n",
    "            Hpref[b][:A] -= step_size * (Rn - mean_reward) * pr_action_t[b][:A] #updates for action 0 to A-1\n",
    "            # if the value of the current reward is higher than the mean value, then the preference estimates for the actions before them will DECREASRE and otherwise it will increase.\n",
    "            Hpref[b][A + 1:] -= step_size * (Rn - mean_reward) * pr_action_t[b][A + 1:] #updates actions for A+1 onwards\n",
    "            #same as above.\n",
    "            Hpref[b][A] += step_size * (Rn - mean_reward) * (1 - pr_action_t[b][A]) #updates action A only                       \n",
    "\n",
    "        #Calculates the percentage of hoosing optimal action in each step for every bandit problem\n",
    "        optimal_choice_per_step.append((sum_of_optimal_choice / n_bandit) * 100)\n",
    "    # Appendsall of the optimal percentage choices for all steps to global_reward_list\n",
    "    optimal_choice_list.append(optimal_choice_per_step) # returned for the second plot\n",
    "\n",
    "    average_rewards = np.mean(rewards,axis = 1)\n",
    "    global_average_reward_list.append(average_rewards) # returned for the first plot\n",
    "    return \n",
    "\n",
    "# settings\n",
    "steps = 1000\n",
    "n_bandit = 1000\n",
    "n_lever = 10\n",
    "initial_reward_estimates = np.zeros((n_bandit, n_lever))\n",
    "testbed = np.random.normal(0, 1, (n_bandit, n_lever))\n",
    "optimal_choice_list = []\n",
    "global_average_reward_list = []\n",
    "baseline_applied = [True, False]  \n",
    "step_size = [0.1,0.01,0.2]  # n_lever values\n",
    "combinations = list(itertools.product(step_size, baseline_applied))\n",
    "\n",
    "for size, baseline in combinations:\n",
    "    \n",
    "    print(f\"Running gradient_bandit with baseline={baseline} and step_size={size}\")\n",
    "    gradient_bandit(steps, n_bandit, n_lever, size, baseline, testbed, global_average_reward_list, optimal_choice_list)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "\n",
    "for i, (size,baseline) in enumerate(combinations):\n",
    "    label = f'(step_size={size}, baseline={baseline})'\n",
    "    plt.plot(global_average_reward_list[i], label=label)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Average Global Rewards')\n",
    "plt.legend()\n",
    "plt.title('Gradient Bandit Algorithm Performance')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "for i, (size,baseline) in enumerate(combinations):\n",
    "    label = f'(step_size={size}, baseline={baseline})'\n",
    "    plt.plot(optimal_choice_list[i], label=label)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Optimal Action (%)')\n",
    "plt.legend()\n",
    "plt.title('Gradient Bandit Algorithm Performance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d981ed0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
